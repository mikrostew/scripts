#!/usr/bin/env bash
# Download a webpage and all its requisites using wget

@arg 'page_url' 'URL of the page to download'

@import do_cmd from .bash_shared_functions

@uses_cmds wget

# explanation of the options:
# --wait=1 - wait 1 second between retrievals, to ease the load on the server
# --random-wait - vary the request time between 0.5 to 1.5 times the --wait seconds
# --convert-links - after download, convert links to be suitable for local viewing
# --page-requisites - download all the files required to display the page
# --user-agent="" - don't send the user-agent header

do_cmd wget --wait=1 --random-wait --convert-links --page-requisites --user-agent="" "$page_url"

